---
title: "Mutagen Prediction"
output: html_document
---

```{r}
start <- function(pkg){
  npkg <- pkg[!(pkg %in% installed.packages()[,"Package"])]
  if (length(npkg))
    install.packages(npkg, dependencies = TRUE)
  lapply(pkg, require, character.only=TRUE)
}

pkgs <- c("ROCR","rcdk","randomForest","MASS","e1071","caret")
start(pkgs)


# Parse smiles structures from the prefiltered dataset --------------------

dat1<-read.csv("mutagendata.smi",sep="\t",header=F) # smile + IDs + Outcomec
# sum(dat1[,3]=="mutagen")/nrow(dat1) # 0.59

smi <-lapply(as.character(dat1$V1),parse.smiles) 
cmp.fp<-vector("list",nrow(dat1))


# Generate and save fingerprints in dataframe ---------------------------------------------------

for (i in 1:nrow(dat1)){
  cmp.fp[i]<-lapply(smi[[i]][1],get.fingerprint,type="maccs")
}

fpmac<-fp.to.matrix(cmp.fp)
# colSums(fpmac)
# fingerprints with the sum of 0 are removed
cmp.finger<-as.data.frame(fpmac[,colSums(fpmac) != 0])

dataset<-cbind(cmp.finger,dat1$V3)
colnames(dataset)[152]<-"Outcome"


# Split dataset on 8:2 into training and test sets ------------------------
mask <- sample(2, nrow(dataset), replace = TRUE, prob=c(0.8,0.2))
training <- dataset[mask == 1,]
test <- dataset[mask == 2,]
```

## Bagging and Random Forest
```{r}
bag <- randomForest(Outcome~., training, mtry = 151, importance=TRUE)
bag

bag.pred <- predict(bag, newdata = test)
plot(bag.pred, test$Outcome)
(sum(bag.pred !=test$Outcome))^2/3514

rf <- randomForest(Outcome~., training, mtry = 75, importance=TRUE)
rf.pred <- predict(rf, newdata = test)
(sum(rf.pred!= test$Outcome))^2/3514
importance(rf)
varImpPlot(rf)
```



## Support Vector Classifier
```{r}
set.seed(1)
costs <- tune(svm, Outcome~., data = training, kernel = "linear", 
              ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10)),
              scale = FALSE)
summary(costs)

bestsvm <- costs$best.model
summary(bestsvm)

svm.pred <- predict(bestsvm, test)
table(Prediction = svm.pred, Truth = test$Outcome)
```

## Stacking

Predictions from random forest and SVM are combined to improve accuracy.

```{r}
bine <- data.frame(rf.pred, svm.pred, Outcome = test$Outcome)
bine.model <- train(Outcome~., bine, method = "rf")
bine.pred <- predict(bine.model, test)
confusionMatrix(test$Outcome, bine.pred)
```

## CV
```{r}
k <- 10 
n= floor(nrow(training)/k)
err.rf<- rep(NA,k) 
err.svm <- rep(NA, k)

for (i in 1:k){
    s1 <- ((i-1)* n+1)
    s2 <- (i*n)
    subset <- s1:s2
    cv.train <- training[-subset,]
    cv.test <- training[subset,]
    
    rf.fit <- randomForest(x=cv.train[1:151],y= cv.train$Outcome)
    rf.pred <- predict(rf.fit,newdata= cv.test[1:151],type<-"prob")
    rf<-prediction(rf.pred[,2],cv.test$Outcome)
    err.rf[i]<-performance(rf,"auc")@y.values[[1]]
    print(paste("AUC of random forest for the fold",i,":",err.rf[i]))
    
    svm.fit <- svm(x=cv.train[1:151],y= cv.train$Outcome, kernel = "linear", cost = 5, scale = FALSE,
                   probability = TRUE)
    svm.pred <- predict(svm.fit, newdata = cv.test[1:151], probability = TRUE, decision.values = TRUE)
    probs <-  attr(svm.pred, "probabilities")[,2] 
    svm <- prediction(probs,cv.test$Outcome)
    err.svm[i] <- performance(svm, "auc")@y.values[[1]]
    print(paste("AUC of SVM for the fold",i,":",err.svm[i]))
}

print(paste("RandomForest Average AUC:",mean(err.rf), "SVM Average AUC:", mean(err.svm)))
```
